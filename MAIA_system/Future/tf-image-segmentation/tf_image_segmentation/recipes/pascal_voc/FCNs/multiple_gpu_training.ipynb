{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model Using Multiple GPU Cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The design of multiple GPU training is inspired by [the official cifar\n",
    "tutorial with multiple gpu option](https://www.tensorflow.org/tutorials/deep_cnn#training_a_model_using_multiple_gpu_cards).\n",
    "\n",
    "To be more precise we do the following:\n",
    "\n",
    "1. We pin all the model variables onto CPU device.\n",
    "2. All operations are pinned onto the GPUs. Each GPUs has a replica of all the operations.\n",
    "3. On each iteration varialbes of the model (weights) are being trasnfered to each GPU device and each device processes a separate batch in parallel.\n",
    "4. After gradients for each batch have been computed, they are being transfered back to the main memory where they are averaged and the model variables are being updated.\n",
    "\n",
    "The decision to store the model parameters on the CPU is caused by the fact that transferring the data between\n",
    "devices is [costly](http://stackoverflow.com/questions/34428850/variables-on-cpu-training-gradients-on-gpu\n",
    "). The parameters are being moved with the help of DMA (Direct memory access). The speed of transfer (according to this [resource](:http://timdettmers.com/2015/03/09/deep-learning-hardware-guide/)) is 12 GB/s. Average model size is 553 MB ([VGG-16](http://stackoverflow.com/questions/28232235/how-to-calculate-the-number-of-parameters-of-convolutional-neural-networkscnns)). This justifies the decision to store model parameters on the main memory. If we store the model on the main memory we have to pass $gpuNumber * 2$ messages after computing gradients. If we would have decided to store parameters on each gpu, we would have to pass $(gpuNumber-1)^2$ messages. We opt for the first option where the number of messages being sent is linear in the number of available gpu cards.\n",
    "\n",
    "In case of segmentation, we minimize the pixel-wise cross entropy loss which we also sum over the whole batch. This loss is separable, meaning that gradient of sum is a sum of gradients. This way we can split our batch into equal sum parts and compute gradients for them in parallel on a separate gpu devices. After that we can sum these gradients and apply them to our variables to update them. The synchronization is performed by tensorflow implicitly (the system waits for all gpus to finish computing gradients before proceeding).\n",
    "\n",
    "Below you can see the example code, which we used to measure the speed up of our training with one gpu card and four gpu cards. No saving of models or logging of errors was performed for simplicity of the experiments. We also provide the speed up plots for our experiments. All the experiments were performed on the FCN-32s model which is based on VGG-16 networks architecture.\n",
    "\n",
    "Below, we demonstrate the approach that we used for our multiple gpu card training, speedup and time plots. All the experiments were held with the batch size equal to 10 (meaning that each gpu processed batch of size of 10 in parallel). We carried out our experiments with 1, 2, 3 and 4 GPU cards. We achieved $3.3$ speedup with $4$ GPUs. To compare the processing time each setup (1, 2, 3 or 4 GPUs) had to process a fixed number of batches (160 in our experiment). We measured the time that it took for each setup to complete the task. It should be stated that we could have gotten a better estimates of speedup if we would have run our experiments for a longer time. Overall, we got resutls that are similar to the [official tensorflow multi-gpu benchmark](https://www.tensorflow.org/performance/benchmarks). Below, you can find the code for 1 and 4 gpu training setup and all the plots. The code has the respective output timing in the cells.\n",
    "\n",
    "![title](multiple_gpu_arch.png)\n",
    "\n",
    "![title](multiple_gpu_time.png)\n",
    "\n",
    "![title](multiple_gpu_speedup.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.6332910061\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import os, sys\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# Add a path to a custom fork of TF-Slim\n",
    "# Get it from here:\n",
    "# https://github.com/warmspringwinds/models/tree/fully_conv_vgg\n",
    "sys.path.append(\"my_models/slim/\")\n",
    "\n",
    "# Add path to the cloned library\n",
    "sys.path.append(\"tf-image-segmentation/\")\n",
    "\n",
    "checkpoints_dir = 'checkpoints'\n",
    "\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "vgg_checkpoint_path = os.path.join(checkpoints_dir, 'vgg_16.ckpt')\n",
    "\n",
    "from tf_image_segmentation.utils.tf_records import read_tfrecord_and_decode_into_image_annotation_pair_tensors\n",
    "from tf_image_segmentation.models.fcn_32s import FCN_32s, extract_vgg_16_mapping_without_fc8\n",
    "\n",
    "from tf_image_segmentation.utils.pascal_voc import pascal_segmentation_lut\n",
    "\n",
    "from tf_image_segmentation.utils.training import get_valid_logits_and_labels\n",
    "\n",
    "from tf_image_segmentation.utils.augmentation import (distort_randomly_image_color,\n",
    "                                                      flip_randomly_left_right_image_with_annotation,\n",
    "                                                      scale_randomly_image_with_annotation_with_fixed_size_output)\n",
    "\n",
    "image_train_size = [384, 384]\n",
    "number_of_classes = 21\n",
    "tfrecord_filename = 'pascal_augmented_train.tfrecords'\n",
    "pascal_voc_lut = pascal_segmentation_lut()\n",
    "class_labels = pascal_voc_lut.keys()\n",
    "\n",
    "# Also represents the number of gpus -- should have enough of them on the current\n",
    "# workstation\n",
    "num_clones = 4\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def average_gradients(tower_grads):\n",
    "    \"\"\"Calculate the average gradient for each shared variable across all towers.\n",
    "    Note that this function provides a synchronization point across all towers.\n",
    "    Args:\n",
    "    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n",
    "      is over individual gradients. The inner list is over the gradient\n",
    "      calculation for each tower.\n",
    "    Returns:\n",
    "     List of pairs of (gradient, variable) where the gradient has been averaged\n",
    "     across all towers.\n",
    "    \"\"\"\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "          # Add 0 dimension to the gradients to represent the tower.\n",
    "          expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "          # Append on a 'tower' dimension which we will average over below.\n",
    "          grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(0, grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads\n",
    "\n",
    "\n",
    "filename_queue = tf.train.string_input_producer(\n",
    "    [tfrecord_filename], num_epochs=1)\n",
    "\n",
    "image, annotation = read_tfrecord_and_decode_into_image_annotation_pair_tensors(filename_queue)\n",
    "\n",
    "# Various data augmentation stages\n",
    "image, annotation = flip_randomly_left_right_image_with_annotation(image, annotation)\n",
    "\n",
    "# image = distort_randomly_image_color(image)\n",
    "\n",
    "resized_image, resized_annotation = scale_randomly_image_with_annotation_with_fixed_size_output(image, annotation, image_train_size)\n",
    "\n",
    "\n",
    "resized_annotation = tf.squeeze(resized_annotation)\n",
    "\n",
    "image_batch, annotation_batch = tf.train.shuffle_batch( [resized_image, resized_annotation],\n",
    "                                             batch_size=10,\n",
    "                                             capacity=3000,\n",
    "                                             num_threads=4,\n",
    "                                             min_after_dequeue=1000)\n",
    "\n",
    "\n",
    "batch_queue = slim.prefetch_queue.prefetch_queue(\n",
    "                  [image_batch, annotation_batch], capacity= 2 * num_clones)\n",
    "\n",
    "variables_device = '/cpu:0'\n",
    "\n",
    "with tf.device(variables_device):\n",
    "    \n",
    "    opt = tf.train.AdamOptimizer(learning_rate=0.000001)\n",
    "\n",
    "    # Puts weights onto cpu\n",
    "    with slim.arg_scope([slim.model_variable, slim.variable],\n",
    "                         device=variables_device):\n",
    "        \n",
    "        tower_grads = []\n",
    "        losses = []\n",
    "        \n",
    "        for gpu_number in xrange(num_clones):\n",
    "            # Get different name for each copy which is being placed\n",
    "            # on separate gpus.\n",
    "            with tf.name_scope('gpu_number_{}_ops'.format(gpu_number)):\n",
    "\n",
    "                # Actually place all operations on gpu\n",
    "                with tf.device('/gpu:{}'.format(gpu_number)):\n",
    "\n",
    "                    images_batch, annotation_batch = batch_queue.dequeue()\n",
    "\n",
    "                    # here we actually want to also define loss\n",
    "                    upsampled_logits_batch, vgg_16_variables_mapping = FCN_32s(image_batch_tensor=images_batch,\n",
    "                                                                               number_of_classes=number_of_classes,\n",
    "                                                                               is_training=True)\n",
    "                    \n",
    "                    # Reuse variables for the next tower.\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                    valid_labels_batch_tensor, valid_logits_batch_tensor = get_valid_logits_and_labels(annotation_batch_tensor=annotation_batch,\n",
    "                                                                                         logits_batch_tensor=upsampled_logits_batch,\n",
    "                                                                                        class_labels=class_labels)\n",
    "\n",
    "                    cross_entropies = tf.nn.softmax_cross_entropy_with_logits(logits=valid_logits_batch_tensor,\n",
    "                                                                              labels=valid_labels_batch_tensor)\n",
    "\n",
    "                    # Normalize the cross entropy -- the number of elements\n",
    "                    # is different during each step due to mask out regions\n",
    "                    cross_entropy_sum = tf.reduce_mean(cross_entropies)\n",
    "\n",
    "                    grad_vars = opt.compute_gradients(cross_entropy_sum)\n",
    "                    \n",
    "                    tower_grads.append(grad_vars)\n",
    "                    losses.append(cross_entropy_sum)\n",
    "                    \n",
    "    averaged_grads_vars = average_gradients(tower_grads)\n",
    "    \n",
    "    averaged_loss = tf.reduce_mean(losses)\n",
    "                \n",
    "    # Here we apply our gradient on the cpu, by\n",
    "    # collecting the gradients from each gpu and averaging them\n",
    "    # and finally applying them at the end\n",
    "    # todo: add global step to count the actual step number\n",
    "    apply_grad_op = opt.apply_gradients(averaged_grads_vars)\n",
    "    \n",
    "    \n",
    "# Variable's initialization functions\n",
    "vgg_16_without_fc8_variables_mapping = extract_vgg_16_mapping_without_fc8(vgg_16_variables_mapping)\n",
    "\n",
    "\n",
    "init_fn = slim.assign_from_checkpoint_fn(model_path=vgg_checkpoint_path,\n",
    "                                         var_list=vgg_16_without_fc8_variables_mapping)\n",
    "\n",
    "global_vars_init_op = tf.global_variables_initializer()\n",
    "\n",
    "#The op for initializing the variables.\n",
    "local_vars_init_op = tf.local_variables_initializer()\n",
    "\n",
    "combined_op = tf.group(local_vars_init_op, global_vars_init_op)\n",
    "\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))  as sess:\n",
    "    \n",
    "    sess.run(combined_op)\n",
    "    init_fn(sess)\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    \n",
    "    start = time.time()    \n",
    "\n",
    "    for i in xrange(40):\n",
    "    \n",
    "        cross_entropy, _ = sess.run([ averaged_loss,\n",
    "                                      apply_grad_op\n",
    "                                    ])\n",
    "        # Loss is going down -- was checked,\n",
    "        # comment out to make sure\n",
    "        # print( str(i) + \" Current loss: \"  + str(cross_entropy))\n",
    "    \n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263.757616043\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import os, sys\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# Add a path to a custom fork of TF-Slim\n",
    "# Get it from here:\n",
    "# https://github.com/warmspringwinds/models/tree/fully_conv_vgg\n",
    "sys.path.append(\"my_models/slim/\")\n",
    "\n",
    "# Add path to the cloned library\n",
    "sys.path.append(\"tf-image-segmentation/\")\n",
    "\n",
    "checkpoints_dir = 'checkpoints'\n",
    "\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "vgg_checkpoint_path = os.path.join(checkpoints_dir, 'vgg_16.ckpt')\n",
    "\n",
    "from tf_image_segmentation.utils.tf_records import read_tfrecord_and_decode_into_image_annotation_pair_tensors\n",
    "from tf_image_segmentation.models.fcn_32s import FCN_32s, extract_vgg_16_mapping_without_fc8\n",
    "\n",
    "from tf_image_segmentation.utils.pascal_voc import pascal_segmentation_lut\n",
    "\n",
    "from tf_image_segmentation.utils.training import get_valid_logits_and_labels\n",
    "\n",
    "from tf_image_segmentation.utils.augmentation import (distort_randomly_image_color,\n",
    "                                                      flip_randomly_left_right_image_with_annotation,\n",
    "                                                      scale_randomly_image_with_annotation_with_fixed_size_output)\n",
    "\n",
    "image_train_size = [384, 384]\n",
    "number_of_classes = 21\n",
    "tfrecord_filename = 'pascal_augmented_train.tfrecords'\n",
    "pascal_voc_lut = pascal_segmentation_lut()\n",
    "class_labels = pascal_voc_lut.keys()\n",
    "\n",
    "# Also represents the number of gpus -- should have enough of them on the current\n",
    "# workstation\n",
    "num_clones = 1\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def average_gradients(tower_grads):\n",
    "    \"\"\"Calculate the average gradient for each shared variable across all towers.\n",
    "    Note that this function provides a synchronization point across all towers.\n",
    "    Args:\n",
    "    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n",
    "      is over individual gradients. The inner list is over the gradient\n",
    "      calculation for each tower.\n",
    "    Returns:\n",
    "     List of pairs of (gradient, variable) where the gradient has been averaged\n",
    "     across all towers.\n",
    "    \"\"\"\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "          # Add 0 dimension to the gradients to represent the tower.\n",
    "          expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "          # Append on a 'tower' dimension which we will average over below.\n",
    "          grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(0, grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads\n",
    "\n",
    "\n",
    "filename_queue = tf.train.string_input_producer(\n",
    "    [tfrecord_filename], num_epochs=1)\n",
    "\n",
    "image, annotation = read_tfrecord_and_decode_into_image_annotation_pair_tensors(filename_queue)\n",
    "\n",
    "# Various data augmentation stages\n",
    "image, annotation = flip_randomly_left_right_image_with_annotation(image, annotation)\n",
    "\n",
    "# image = distort_randomly_image_color(image)\n",
    "\n",
    "resized_image, resized_annotation = scale_randomly_image_with_annotation_with_fixed_size_output(image, annotation, image_train_size)\n",
    "\n",
    "\n",
    "resized_annotation = tf.squeeze(resized_annotation)\n",
    "\n",
    "image_batch, annotation_batch = tf.train.shuffle_batch( [resized_image, resized_annotation],\n",
    "                                             batch_size=10,\n",
    "                                             capacity=3000,\n",
    "                                             num_threads=4,\n",
    "                                             min_after_dequeue=1000)\n",
    "\n",
    "\n",
    "batch_queue = slim.prefetch_queue.prefetch_queue(\n",
    "                  [image_batch, annotation_batch], capacity= 2 * num_clones)\n",
    "\n",
    "variables_device = '/cpu:0'\n",
    "\n",
    "with tf.device(variables_device):\n",
    "    \n",
    "    opt = tf.train.AdamOptimizer(learning_rate=0.000001)\n",
    "\n",
    "    # Puts weights onto cpu\n",
    "    with slim.arg_scope([slim.model_variable, slim.variable],\n",
    "                         device=variables_device):\n",
    "        \n",
    "        tower_grads = []\n",
    "        losses = []\n",
    "        \n",
    "        for gpu_number in xrange(num_clones):\n",
    "            # Get different name for each copy which is being placed\n",
    "            # on separate gpus.\n",
    "            with tf.name_scope('gpu_number_{}_ops'.format(gpu_number)):\n",
    "\n",
    "                # Actually place all operations on gpu\n",
    "                with tf.device('/gpu:{}'.format(gpu_number)):\n",
    "\n",
    "                    images_batch, annotation_batch = batch_queue.dequeue()\n",
    "\n",
    "                    # here we actually want to also define loss\n",
    "                    upsampled_logits_batch, vgg_16_variables_mapping = FCN_32s(image_batch_tensor=images_batch,\n",
    "                                                                               number_of_classes=number_of_classes,\n",
    "                                                                               is_training=True)\n",
    "                    \n",
    "                    # Reuse variables for the next tower.\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                    valid_labels_batch_tensor, valid_logits_batch_tensor = get_valid_logits_and_labels(annotation_batch_tensor=annotation_batch,\n",
    "                                                                                         logits_batch_tensor=upsampled_logits_batch,\n",
    "                                                                                        class_labels=class_labels)\n",
    "\n",
    "                    cross_entropies = tf.nn.softmax_cross_entropy_with_logits(logits=valid_logits_batch_tensor,\n",
    "                                                                              labels=valid_labels_batch_tensor)\n",
    "\n",
    "                    # Normalize the cross entropy -- the number of elements\n",
    "                    # is different during each step due to mask out regions\n",
    "                    cross_entropy_sum = tf.reduce_mean(cross_entropies)\n",
    "\n",
    "                    grad_vars = opt.compute_gradients(cross_entropy_sum)\n",
    "                    \n",
    "                    tower_grads.append(grad_vars)\n",
    "                    losses.append(cross_entropy_sum)\n",
    "                    \n",
    "    averaged_grads_vars = average_gradients(tower_grads)\n",
    "    \n",
    "    averaged_loss = tf.reduce_mean(losses)\n",
    "                \n",
    "    # Here we apply our gradient on the cpu, by\n",
    "    # collecting the gradients from each gpu and averaging them\n",
    "    # and finally applying them at the end\n",
    "    # todo: add global step to count the actual step number\n",
    "    apply_grad_op = opt.apply_gradients(averaged_grads_vars)\n",
    "    \n",
    "    \n",
    "# Variable's initialization functions\n",
    "vgg_16_without_fc8_variables_mapping = extract_vgg_16_mapping_without_fc8(vgg_16_variables_mapping)\n",
    "\n",
    "\n",
    "init_fn = slim.assign_from_checkpoint_fn(model_path=vgg_checkpoint_path,\n",
    "                                         var_list=vgg_16_without_fc8_variables_mapping)\n",
    "\n",
    "global_vars_init_op = tf.global_variables_initializer()\n",
    "\n",
    "#The op for initializing the variables.\n",
    "local_vars_init_op = tf.local_variables_initializer()\n",
    "\n",
    "combined_op = tf.group(local_vars_init_op, global_vars_init_op)\n",
    "\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))  as sess:\n",
    "    \n",
    "    sess.run(combined_op)\n",
    "    init_fn(sess)\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    \n",
    "    start = time.time()    \n",
    "\n",
    "    for i in xrange(160):\n",
    "    \n",
    "        cross_entropy, _ = sess.run([ averaged_loss,\n",
    "                                      apply_grad_op\n",
    "                                    ])\n",
    "        # Loss is going down -- was checked,\n",
    "        # comment out to make sure\n",
    "        # print( str(i) + \" Current loss: \"  + str(cross_entropy))\n",
    "    \n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
